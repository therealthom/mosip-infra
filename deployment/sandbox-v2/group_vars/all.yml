
---
# Global variables - accessed by different roles and plays
#
tmp_dir: '/tmp/mosip' 
logs_dir: '{{tmp_dir}}/logs'
user_home: '{{lookup("env", "HOME")}}'
install_root: '{{user_home}}/mosip-infra/deployment/sandbox-v2'
charts_root: '{{install_root}}/helm/charts'  # Helm charts root
helm_cli_path: '{{user_home}}/bin'  # This path chosen as it is included in default $PATH in Centos 7.7
artifactory_url: http://15.207.222.178

sandbox_domain_name: mosip.nuuptech.cloud
site:
  sandbox_public_url: 'https://{{sandbox_domain_name}}'
  ssl:
    get_certificate: true  # get a fresh certificate for the domain using Letsencrypt.
    email: info@mosip.io
    certificate: '/etc/letsencrypt/live/{{sandbox_domain_name}}/fullchain.pem'
    certificate_key: '/etc/letsencrypt/live/{{sandbox_domain_name}}/privkey.pem'
   
is_glowroot: absent  # absent or present

docker_wait_time: 600   # Wait for docker to pull and deploy

clusters:
  mz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/mzcluster.config" 
    nodeport_node: ip162.ip-144-217-96.net  # Any node on cluster for nodeport access
    any_node_ip: '144.217.96.162' # ip address of above node
    ingress:
      namespace: ingress-nginx
      nodeports:
        http: 30080 
        https: 30443
      base_url: 'http://{{groups["mzworkers"][0]}}:30080' # Any node since ingress runs on nodeport
    dashboard:
      url: /mz-dashboard
      token_file: '{{tmp_dir}}/dashboard_mz.token'
      token_expiry: 86400 # Seconds
      nodeport: 30081  # Dashboard runs on nodeport 
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/mz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/mz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/mz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/mz/grafana'
      grafana_ingress_path: 'mz-grafana'
      grafana_token_file: '{{tmp_dir}}/grafana_mz.token'
      elasticsearch:
        host: 'elasticsearch-master:9200'
      kibana: 
        url: http://kibana-kibana:5601 
  dmz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/dmzcluster.config" 
    nodeport_node: 	ip172.ip-144-217-96.net  # Any node on cluster for nodeport access
    any_node_ip: '144.217.96.172' # ip adddress of above node
    ingress:
      namespace: ingress-nginx
      nodeports:
        http: 30080 
        https: 30443
      base_url: 'http://{{groups["dmzworkers"][0]}}:30080' # Any node since ingress runs on nodeport
    dashboard: 
      url: /dmz-dashboard
      token_file: '{{tmp_dir}}/dashboard_dmz.token'
      token_expiry: 86400 # Seconds 
      nodeport: 30081  # Dashboard runs on nodeport 
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/dmz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/dmz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/dmz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/dmz/grafana'
      grafana_ingress_path: 'dmz-grafana'
      grafana_token_file: '{{tmp_dir}}/grafana_dmz.token'
      elasticsearch:
        host: 'mzworker0.sb:30080/elasticsearch' # Connect to elasticsearch on MZ
      kibana: 
        url: http://mzworker0.sb:30601  # Kibana runs on this nodeport on MZ. 

# Registration processor shared variables
#
regproc:
  nfs:
    server: '{{nfs.server}}'
    landing_folder: '{{nfs.folder}}/landing'

# Postgres persistent storage
postgres:
  nfs_path: '{{nfs.folder}}/postgres'  
  size: 10Gi
  service_name: postgres # svc on Kubernetes
  service_port: 80
  server: '{{clusters.mz.nodeport_node}}'
  nodeport: 30090  # fixed for connecting from outside the cluster
  user: postgres
  password: postgres
  max_connections: 1000
  node_affinity: 
    enabled: false # To run postgres on an exclusive node
    node: 'mzworker0.sb' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "postgres" # Key for applying taint on node
      value: "only"  

# Keycloak
keycloak:
  user: admin
  password: admin
  url: '{{clusters.mz.ingress.base_url}}/keycloak'

softhsm:
  keymanager:  
    label: keymanager  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/keymanager/'
  ida:  # Auth
    label: ida  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/ida/'

repos:
  commons:
    src: https://github.com/mosip/commons 
    dest: '{{tmp_dir}}/commons'
    tag: 1.0.9
  prereg:
    src: https://github.com/mosip/pre-registration
    dest: '{{tmp_dir}}/pre-registration'
    tag: master
  regproc:
    src: https://github.com/mosip/registration
    dest: '{{tmp_dir}}/registration'
    tag: master
  ida:
    src: https://github.com/mosip/id-authentication
    dest: '{{tmp_dir}}/id-authentication'
    tag: 1.0.9
  pms:
    src: https://github.com/mosip/partner-management-services
    dest: '{{tmp_dir}}/partner-management-services'
    tag: master

# DBs 
#
databases:
  mosip_kernel:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_master:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_iam:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_audit:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_idrepo:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_idmap:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_prereg:
    sql_path: '{{repos.prereg.dest}}/db_scripts'
  mosip_regprc:
    sql_path: '{{repos.regproc.dest}}/db_scripts'
  mosip_ida:
    sql_path: '{{repos.ida.dest}}/db_scripts'
  mosip_pmp:
    sql_path: '{{repos.pms.dest}}/db_scripts'

nfs:
  server: console.sb
  folder: /srv/nfs/mosip


hdfs:
  install_name: hadoop
  users:  # Must match names in property files
    - regprocessor
    - prereg
    - idrepo 
  user_base_dir: /user
  namenode_pod_name: 'hadoop-hdfs-nn-0'  
  nfs:
    server: '{{nfs.server}}'
    nn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/nn'  
    dn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/dn' 
  node_affinity: 
    enabled: false # To run all hdfs pods exclusively on a single node.
    node: 'mzworker1.sb' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "hdfs" # Key for applying taint on node
      value: "only"  

config_repo:
  name: config_repo  # Git repo inside above node_path
  path: '{{nfs.folder}}/config_repo'
  search_folder: ''  # If all properites at at the root repo level, then this will be empty
  storage_size: 10Mi

activemq:
  name: activemq
  nodeport: 30616  # Arbitrary
  container_port: 61616 
